import json
import logging
from typing import Dict, List, Any, Tuple
from tenacity import retry, stop_after_attempt, wait_exponential
import google.generativeai as genai
from .events import EventManager

# Logger setup
logger = logging.getLogger(__name__)

@retry(
    wait=wait_exponential(multiplier=1, min=4, max=10),
    stop=stop_after_attempt(3),
    reraise=True
)
def generate_response_with_analysis(
    model: genai.GenerativeModel,
    system_prompt: str,
    chat_history: List[Dict[str, str]],
    user_message: str
) -> Tuple[str, Dict[str, int]]:
    """
    Get AI response and personality analysis in a single API call.
    Reduces cost and latency by 50%.

    Args:
        model: GenerativeModel instance to use.
        system_prompt: System prompt defining AI's role and personality.
        chat_history: List of previous conversation history.
        user_message: User's latest message.

    Returns:
        Tuple (ai_response, analysis_scores).
        ai_response: Response message generated by AI.
        analysis_scores: Dictionary of personality analysis scores.
    
    Raises:
        Exception: If API call fails after 3 retries.
    """
    logger.info("Generating AI response with personality analysis in single call...")
    
    # Event manager for context
    event_manager = EventManager()
    event_prompt = event_manager.get_event_prompt_modifiers()
    
    # Combined prompt with event context
    combined_prompt = f"""
    {system_prompt}

    # Current Context and Events
    {event_prompt}

    Respond to the following user message and perform personality analysis simultaneously.

    User message: "{user_message}"

    Response format must strictly follow this JSON format:
    {{
        "response": "Your response message here",
        "personality_scores": {{
            "tsundere": 0-10 integer,
            "yandere": 0-10 integer,
            "kuudere": 0-10 integer,
            "dandere": 0-10 integer
        }}
    }}

    Important:
    - Response message should be natural based on your personality and current context
    - personality_scores should analyze the personality tendencies in the user message
    - Do not output any text outside the JSON format
    """
    
    try:
        # Combine conversation history with combined prompt
        full_prompt = [
            {'role': 'user', 'parts': [system_prompt]},
            {'role': 'model', 'parts': ["Yes, I understand. I will respond with these settings."]}
        ] + chat_history + [{'role': 'user', 'parts': [combined_prompt]}]

        response = model.generate_content(full_prompt)
        
        if not response.text:
            logger.error("Empty response from Gemini API")
            return "Sorry, I couldn't generate a response.", {"tsundere": 0, "yandere": 0, "kuudere": 0, "dandere": 0}
        
        # Parse JSON response
        json_text = response.text.strip()
        
        # Remove code blocks
        if '```json' in json_text:
            json_text = json_text.split('```json')[1].split('```')[0]
        elif '```' in json_text:
            json_text = json_text.split('```')[1].split('```')[0]
        
        result = json.loads(json_text)
        
        # Validate required fields
        if 'response' not in result or 'personality_scores' not in result:
            logger.warning(f"Missing required fields in response: {result}")
            return "Response format is incorrect.", {"tsundere": 0, "yandere": 0, "kuudere": 0, "dandere": 0}
        
        # Score validation and defaults
        default_scores = {"tsundere": 0, "yandere": 0, "kuudere": 0, "dandere": 0}
        scores = result['personality_scores']
        
        for key in default_scores:
            if key not in scores or not isinstance(scores[key], int):
                logger.warning(f"Invalid score for '{key}': {scores.get(key)}")
                scores[key] = default_scores[key]
            # Clip scores to 0-10 range
            scores[key] = max(0, min(10, scores[key]))
        
        return result['response'], scores

    except json.JSONDecodeError as e:
        logger.error(f"Failed to decode JSON from combined response: {e}. Response text: {response.text}")
        # Fallback: normal response generation
        return generate_response_fallback(model, system_prompt, chat_history, user_message)
    except Exception as e:
        logger.error(f"Error during combined API call: {e}", exc_info=True)
        # Fallback
        return generate_response_fallback(model, system_prompt, chat_history, user_message)

def generate_response_fallback(
    model: genai.GenerativeModel,
    system_prompt: str,
    chat_history: List[Dict[str, str]],
    user_message: str
) -> Tuple[str, Dict[str, int]]:
    """
    Fallback function when combined API call fails.
    Reverts to original two-call method.
    """
    logger.warning("Using fallback method (two API calls)")
    
    try:
        # Event manager for context
        event_manager = EventManager()
        event_prompt = event_manager.get_event_prompt_modifiers()
        
        # Enhanced system prompt with events
        enhanced_system_prompt = f"{system_prompt}\n\n# Current Context\n{event_prompt}" if event_prompt else system_prompt
        
        # Normal response generation
        full_prompt = [
            {'role': 'user', 'parts': [enhanced_system_prompt]},
            {'role': 'model', 'parts': ["Yes, I understand. I will respond with these settings."]}
        ] + chat_history + [{'role': 'user', 'parts': [user_message]}]

        response = model.generate_content(full_prompt)
        ai_response = response.text if response.text else "Sorry, I couldn't generate a response."
        
        # Personality analysis
        analysis_result = analyze_personality_scores_fallback(model, user_message)
        
        return ai_response, analysis_result
        
    except Exception as e:
        logger.error(f"Fallback method also failed: {e}")
        return "An error occurred. Please try again later.", {"tsundere": 0, "yandere": 0, "kuudere": 0, "dandere": 0}

def analyze_personality_scores_fallback(
    model: genai.GenerativeModel,
    user_message: str
) -> Dict[str, int]:
    """
    Fallback personality analysis function.
    """
    analysis_prompt = f"""
    Analyze the following user message and evaluate the user's potential preference for 4 personality traits using integer scores from 0 to 10.
    Output must be only in the following JSON format.

    User message: "{user_message}"

    JSON output:
    {{
        "tsundere": <0-10 integer>,
        "yandere": <0-10 integer>,
        "kuudere": <0-10 integer>,
        "dandere": <0-10 integer>
    }}
    """
    
    default_scores = {"tsundere": 0, "yandere": 0, "kuudere": 0, "dandere": 0}

    try:
        response = model.generate_content(analysis_prompt)
        
        if not response.text:
            return default_scores
            
        json_text = response.text.strip()
        if '```json' in json_text:
            json_text = json_text.split('```json')[1].split('```')[0]
        
        scores = json.loads(json_text)
        
        # Validate scores are as expected
        for key in default_scores:
            if key not in scores or not isinstance(scores[key], int):
                logger.warning(f"Invalid or missing key '{key}' in fallback analysis response.")
                return default_scores
        
        return scores

    except Exception as e:
        logger.error(f"Error during fallback personality analysis: {e}")
        return default_scores